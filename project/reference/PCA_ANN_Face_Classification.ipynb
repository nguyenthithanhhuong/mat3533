{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5aa333cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 77760 243 320\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# path to the database - change it if needed\n",
    "path = 'D:\\\\Teach_n_Train\\\\Machine Learning\\\\code\\\\yale\\\\yalefaces_data\\\\' \n",
    "\n",
    "ids = range(1, 16) # 15 people\n",
    "states = ['centerlight', 'glasses', 'happy', 'leftlight', \n",
    "          'noglasses', 'normal', 'rightlight','sad', \n",
    "          'sleepy', 'surprised', 'wink' ]\n",
    "prefix = 'subject'\n",
    "surfix = '.png' #file extension is png\n",
    "\n",
    "# open one picture to get the image's size\n",
    "fn = prefix + '01.' + states[0] + surfix\n",
    "im = cv2.imread(path + fn, 0)\n",
    "\n",
    "h = im.shape[0] # hight \n",
    "w = im.shape[1] # width\n",
    "\n",
    "D = h * w\n",
    "N = len(states)*15 \n",
    "print(N, D, h, w)\n",
    "\n",
    "X = np.zeros((D, N))\n",
    "Y = np.zeros(N)\n",
    "\n",
    "# collect all data\n",
    "cnt = 0 \n",
    "\n",
    "# there are 15 people\n",
    "for person_id in range(1, 16):\n",
    "    for state in states:\n",
    "        \n",
    "        # get name of each image file\n",
    "        fn = path + prefix + str(person_id).zfill(2) + '.' + state + surfix\n",
    "        \n",
    "        # open the file and read as grey image\n",
    "        tmp = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # then add image to dataset X\n",
    "        X[:, cnt] = tmp.reshape(D)\n",
    "        Y[cnt] = states.index(state)\n",
    "        \n",
    "        cnt += 1 \n",
    "Y = Y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dc2d21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=125)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing PCA, note that each row is a datapoint\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# remain dim. k = 125 - change it! \n",
    "pca = PCA(n_components=125) \n",
    "\n",
    "# then apply to data X\n",
    "pca.fit(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370414d",
   "metadata": {},
   "source": [
    "###### Use PCA to find (norm) orthogonal bases  U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "962009d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77760, 125)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then build projection matrix \n",
    "U = pca.components_.T\n",
    "U.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f9d48",
   "metadata": {},
   "source": [
    "###### Project X to less-dimention sub-space: Z = U^T . X^\n",
    "####### Use ANN classifier to solve classification of Z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07b13892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 165)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xhat = np.zeros((D, N))\n",
    "x_mean = X.mean(1)\n",
    "for i in range(N):\n",
    "    Xhat[:,i] = X[:,i] - x_mean[:]\n",
    "    \n",
    "    \n",
    "# Reduced dim. data Z (project of Xhat onto sub-space by Uk - bases)\n",
    "Z = U.T.dot(Xhat)\n",
    "Z.shape    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3788c6c",
   "metadata": {},
   "source": [
    "###### Build ANN from numpy\n",
    "####### Next, we build a Multi Layers Perceptron with 01 hidden layer, wich has dim.=100.\n",
    "####### All of the codes are already shown in the lessons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58c9650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# one-hot coding\n",
    "from scipy import sparse\n",
    "def convert_labels(y, C):\n",
    "    Y = sparse.coo_matrix((np.ones_like(y),\n",
    "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
    "    return Y\n",
    "\n",
    "#softmax for multi-class\n",
    "def softmax(V):\n",
    "    e_V = np.exp(V - np.max(V, axis = 0, keepdims = True))\n",
    "    #print(np.max(V, axis = 0, keepdims = True))\n",
    "    Z = e_V / e_V.sum(axis = 0)\n",
    "    return Z\n",
    "\n",
    "#definition of ReLU, or you can use maximum directly\n",
    "def ReLU(V):\n",
    "    return np.maximum(V, 0)\n",
    "\n",
    "# cost or loss function\n",
    "def cost(Y, Yhat):\n",
    "    return -np.sum(Y*np.log(Yhat))/Y.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75a47535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Loop\n",
    "# To use this code easily, we put it to a method\n",
    "\n",
    "def ANN_3layer_SolveClassification(X_train, Y_train, eta, max_count, num):\n",
    "    X, Y = X_train, Y_train\n",
    "    d, N = X_train.shape\n",
    "    C = Y_train.shape[0]\n",
    "    d1, d2 = num, C\n",
    "    # make random data\n",
    "    W1 = 0.01*np.random.randn(d, d1)\n",
    "    b1 = np.zeros((d1, 1))\n",
    "    W2 = 0.01*np.random.randn(d1, d2)\n",
    "    b2 = np.zeros((d2, 1))\n",
    "\n",
    "    for i in range(max_count + 1):\n",
    "        ## Feedforward\n",
    "        Z1 = np.dot(W1.T, X) + b1\n",
    "        A1 = ReLU(Z1)\n",
    "        Z2 = np.dot(W2.T, A1) + b2\n",
    "        Yhat = softmax(Z2)\n",
    "        \n",
    "          \n",
    "        # print loss after each 10 iterations\n",
    "        if i % 100 == 0:\n",
    "            loss = cost(Y, Yhat)\n",
    "            print(\"iter %d, loss: %f\" %(i, loss))\n",
    "            #print(i)\n",
    "        # backpropagation\n",
    "        E2 = (Yhat - Y )/N\n",
    "        dW2 = np.dot(A1, E2.T)\n",
    "        db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "        E1 = np.dot(W2, E2)\n",
    "        E1[Z1 <= 0] = 0 # gradient of ReLU\n",
    "        dW1 = np.dot(X, E1.T)\n",
    "        db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "\n",
    "        # Gradient Descent update\n",
    "        W1 += -eta*dW1\n",
    "        b1 += -eta*db1\n",
    "        W2 += -eta*dW2\n",
    "        b2 += -eta*db2\n",
    "\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "\n",
    "def predict(W1, W2, b1, b2, images):\n",
    "    Z1 = np.dot(W1.T, images) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    a = softmax(Z2)\n",
    "    #return  np.argmax(Z2, axis=0) #\n",
    "    return np.argmax(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ff4cb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 156)\n",
      "(11, 156)\n",
      "iter 0, loss: 42.086779\n",
      "iter 100, loss: 18.094975\n",
      "iter 200, loss: 11.286700\n",
      "iter 300, loss: 7.716053\n",
      "iter 400, loss: 5.445990\n",
      "iter 500, loss: 3.805930\n",
      "iter 600, loss: 2.664934\n",
      "iter 700, loss: 1.855474\n",
      "iter 800, loss: 1.297671\n",
      "iter 900, loss: 0.922753\n",
      "iter 1000, loss: 0.669344\n",
      "iter 1100, loss: 0.499479\n",
      "iter 1200, loss: 0.386965\n",
      "iter 1300, loss: 0.316331\n",
      "iter 1400, loss: 0.277487\n",
      "iter 1500, loss: 0.252533\n",
      "iter 1600, loss: 0.234557\n",
      "iter 1700, loss: 0.221101\n",
      "iter 1800, loss: 0.210217\n",
      "iter 1900, loss: 0.201245\n",
      "iter 2000, loss: 0.193670\n",
      "iter 2100, loss: 0.187154\n",
      "iter 2200, loss: 0.181484\n",
      "iter 2300, loss: 0.176498\n",
      "iter 2400, loss: 0.172069\n",
      "iter 2500, loss: 0.168109\n",
      "iter 2600, loss: 0.164546\n",
      "iter 2700, loss: 0.161268\n",
      "iter 2800, loss: 0.158270\n",
      "iter 2900, loss: 0.155563\n",
      "iter 3000, loss: 0.153080\n",
      "iter 3100, loss: 0.150790\n",
      "iter 3200, loss: 0.148669\n",
      "iter 3300, loss: 0.146709\n",
      "iter 3400, loss: 0.144884\n",
      "iter 3500, loss: 0.143179\n",
      "iter 3600, loss: 0.141584\n",
      "iter 3700, loss: 0.140086\n",
      "iter 3800, loss: 0.138684\n",
      "iter 3900, loss: 0.137361\n",
      "iter 4000, loss: 0.136112\n"
     ]
    }
   ],
   "source": [
    "#80% for training set - You can change this rate by yourselves\n",
    "M = (int)(Z.shape[1]*0.95)\n",
    "X_train = Z[:, :M]\n",
    "Y_train = convert_labels(Y[:M], 11)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "W1, W2, b1, b2 = ANN_3layer_SolveClassification(X_train, Y_train, 1e-6,4000, num = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ee36431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy training data:  0.9423076923076923\n",
      "(125, 9) (9,) accuracy validation data:  0.3333333333333333\n",
      "[ 2  3  4  5  6  7  8  9 10]\n",
      "[5 3 2 5 6 5 2 7 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score # for evaluating results\n",
    "\n",
    "Y_pred_train = predict(W1, W2, b1, b2, X_train)\n",
    "print('accuracy training data: ', accuracy_score(Y[:M], Y_pred_train))\n",
    "\n",
    "X_val = Z[:, M:]\n",
    "Y_val = convert_labels(Y[M:], 11)\n",
    "\n",
    "\n",
    "Y_pred_val = predict(W1, W2, b1, b2, X_val)\n",
    "print(X_val.shape, Y_pred_val.shape, 'accuracy validation data: ', accuracy_score(Y[M:], Y_pred_val))\n",
    "print(Y[M:])\n",
    "print(Y_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f6232",
   "metadata": {},
   "source": [
    "###### Classify with naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5cc3129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy score using Naive Bayes:  0.8653846153846154\n",
      "Validation Accuracy score using Naive Bayes:  0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB_model = GaussianNB()\n",
    "NB_model.fit(X_train.T, Y[:M].T)\n",
    "\n",
    "Y_predict_nb = NB_model.predict(X_train.T)\n",
    "accuracy_nb = accuracy_score(Y[:M].T, Y_predict_nb)\n",
    "print(\"Train Accuracy score using Naive Bayes: \", accuracy_nb)\n",
    "\n",
    "Y_predict_val_nb = NB_model.predict(X_val.T)\n",
    "accuracy_val_nb = accuracy_score(Y[M:].T, Y_predict_val_nb)\n",
    "print(\"Validation Accuracy score using Naive Bayes: \", accuracy_val_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470d2677",
   "metadata": {},
   "source": [
    "###### Classify with MultiNomial Logistic (SoftMax) Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98385c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy score using MultiNomial Logistic Reg.:  0.9423076923076923\n",
      "Train Accuracy score using MultiNomial Logistic Reg.:  0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lorg=LogisticRegression(multi_class='multinomial',solver='sag', max_iter=5000, penalty='none')\n",
    "lorg.fit(X_train.T,Y[:M].T)\n",
    "\n",
    "Y_pred_softmax =lorg.predict(X_train.T)\n",
    "accuracy_LSM = accuracy_score(Y[:M].T, Y_pred_softmax)\n",
    "print(\"Train Accuracy score using MultiNomial Logistic Reg.: \", accuracy_LSM)\n",
    "\n",
    "Y_pred_val_softmax =lorg.predict(X_val.T)\n",
    "accuracy_val_LSM = accuracy_score(Y[M:].T, Y_pred_val_softmax)\n",
    "print(\"Train Accuracy score using MultiNomial Logistic Reg.: \", accuracy_val_LSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc5befc",
   "metadata": {},
   "source": [
    "###### Use MLPClassifier (Multi Layers Perceptron - Classifier) from Sklearn lib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5db461d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "ann = MLPClassifier()\n",
    "\n",
    "ann.fit(Z.T, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f473c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  0,  1,  2,  3,  5,  5,\n",
       "        6,  7,  8,  9, 10,  0,  1,  2,  3,  5,  5,  6,  7,  8,  9, 10,  0,\n",
       "        1,  2,  3,  4,  7,  6,  7,  8,  9, 10,  0,  1,  2,  3,  4,  5,  6,\n",
       "        7,  8,  9, 10,  0,  1,  2,  3,  5,  5,  6,  7,  8,  9, 10,  0,  1,\n",
       "        2,  3,  4,  4,  6,  7,  8,  9, 10,  0,  5,  2,  3,  4,  5,  6,  7,\n",
       "        8,  9, 10,  0,  1,  2,  3,  5,  5,  6,  7,  8,  9, 10,  0,  1,  2,\n",
       "        3,  4,  5,  6,  7,  8,  9, 10,  0,  1,  2,  3,  4,  5,  6,  7,  8,\n",
       "        9, 10,  0,  1,  2,  3,  4,  4,  6,  7,  8,  9, 10,  0,  1,  2,  3,\n",
       "        4,  5,  6,  7,  8,  9, 10,  0,  1,  2,  3,  5,  5,  6,  7,  8,  9,\n",
       "       10,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ann.predict(Z.T)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55e72c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  0,  1,  2,  3,  4,  5,\n",
       "        6,  7,  8,  9, 10,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  0,\n",
       "        1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  0,  1,  2,  3,  4,  5,  6,\n",
       "        7,  8,  9, 10,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  0,  1,\n",
       "        2,  3,  4,  5,  6,  7,  8,  9, 10,  0,  1,  2,  3,  4,  5,  6,  7,\n",
       "        8,  9, 10,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  0,  1,  2,\n",
       "        3,  4,  5,  6,  7,  8,  9, 10,  0,  1,  2,  3,  4,  5,  6,  7,  8,\n",
       "        9, 10,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  0,  1,  2,  3,\n",
       "        4,  5,  6,  7,  8,  9, 10,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9,\n",
       "       10,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d577937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9454545454545454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score # for evaluating results\n",
    "print(accuracy_score(Y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2043299c89c8cd0b4d1a6f5cf4529bd58e6a4e0fe3181a25e0d328c821cdc5c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
